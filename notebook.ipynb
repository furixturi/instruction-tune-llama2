{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instruction-tune Llama 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: Philipp Schmid https://www.philschmid.de/instruction-tune-llama-2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==4.31.0\n",
      "  Downloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m101.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hCollecting datasets==2.13.0\n",
      "  Downloading datasets-2.13.0-py3-none-any.whl (485 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.6/485.6 kB\u001b[0m \u001b[31m71.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting peft==0.4.0\n",
      "  Downloading peft-0.4.0-py3-none-any.whl (72 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/72.9 kB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting accelerate==0.21.0\n",
      "  Downloading accelerate-0.21.0-py3-none-any.whl (244 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.2/244.2 kB\u001b[0m \u001b[31m52.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting bitsandbytes==0.40.2\n",
      "  Downloading bitsandbytes-0.40.2-py3-none-any.whl (92.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.5/92.5 MB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting trl==0.4.7\n",
      "  Downloading trl-0.4.7-py3-none-any.whl (77 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.4/77.4 kB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting safetensors>=0.3.1\n",
      "  Downloading safetensors-0.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m120.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting filelock (from transformers==4.31.0)\n",
      "  Downloading filelock-3.12.2-py3-none-any.whl (10 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers==4.31.0)\n",
      "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m59.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting numpy>=1.17 (from transformers==4.31.0)\n",
      "  Downloading numpy-1.25.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m131.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from transformers==4.31.0) (23.0)\n",
      "Collecting pyyaml>=5.1 (from transformers==4.31.0)\n",
      "  Downloading PyYAML-6.0.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (757 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m757.7/757.7 kB\u001b[0m \u001b[31m85.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting regex!=2019.12.17 (from transformers==4.31.0)\n",
      "  Downloading regex-2023.6.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (781 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.9/781.9 kB\u001b[0m \u001b[31m100.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from transformers==4.31.0) (2.29.0)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.31.0)\n",
      "  Downloading tokenizers-0.13.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m131.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from transformers==4.31.0) (4.65.0)\n",
      "Collecting pyarrow>=8.0.0 (from datasets==2.13.0)\n",
      "  Downloading pyarrow-12.0.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.9/38.9 MB\u001b[0m \u001b[31m72.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting dill<0.3.7,>=0.3.0 (from datasets==2.13.0)\n",
      "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m34.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pandas (from datasets==2.13.0)\n",
      "  Downloading pandas-2.0.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.2/12.2 MB\u001b[0m \u001b[31m174.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting xxhash (from datasets==2.13.0)\n",
      "  Downloading xxhash-3.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m43.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting multiprocess (from datasets==2.13.0)\n",
      "  Downloading multiprocess-0.70.15-py311-none-any.whl (135 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.4/135.4 kB\u001b[0m \u001b[31m42.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting fsspec[http]>=2021.11.1 (from datasets==2.13.0)\n",
      "  Downloading fsspec-2023.6.0-py3-none-any.whl (163 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.8/163.8 kB\u001b[0m \u001b[31m41.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting aiohttp (from datasets==2.13.0)\n",
      "  Downloading aiohttp-3.8.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m111.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: psutil in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from peft==0.4.0) (5.9.0)\n",
      "Collecting torch>=1.13.0 (from peft==0.4.0)\n",
      "  Downloading torch-2.0.1-cp311-cp311-manylinux1_x86_64.whl (619.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting attrs>=17.3.0 (from aiohttp->datasets==2.13.0)\n",
      "  Downloading attrs-23.1.0-py3-none-any.whl (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.2/61.2 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from aiohttp->datasets==2.13.0) (2.0.4)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets==2.13.0)\n",
      "  Downloading multidict-6.0.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (117 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.4/117.4 kB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3 (from aiohttp->datasets==2.13.0)\n",
      "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets==2.13.0)\n",
      "  Downloading yarl-1.9.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (282 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m282.8/282.8 kB\u001b[0m \u001b[31m54.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting frozenlist>=1.1.1 (from aiohttp->datasets==2.13.0)\n",
      "  Downloading frozenlist-1.4.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (250 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m250.5/250.5 kB\u001b[0m \u001b[31m53.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting aiosignal>=1.1.2 (from aiohttp->datasets==2.13.0)\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.31.0) (4.7.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from requests->transformers==4.31.0) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from requests->transformers==4.31.0) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from requests->transformers==4.31.0) (2023.7.22)\n",
      "Collecting sympy (from torch>=1.13.0->peft==0.4.0)\n",
      "  Downloading sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m179.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting networkx (from torch>=1.13.0->peft==0.4.0)\n",
      "  Downloading networkx-3.1-py3-none-any.whl (2.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m145.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting jinja2 (from torch>=1.13.0->peft==0.4.0)\n",
      "  Downloading Jinja2-3.1.2-py3-none-any.whl (133 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.1/133.1 kB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch>=1.13.0->peft==0.4.0)\n",
      "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m117.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.7.99 (from torch>=1.13.0->peft==0.4.0)\n",
      "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m73.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-cupti-cu11==11.7.101 (from torch>=1.13.0->peft==0.4.0)\n",
      "  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m175.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cudnn-cu11==8.5.0.96 (from torch>=1.13.0->peft==0.4.0)\n",
      "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cublas-cu11==11.10.3.66 (from torch>=1.13.0->peft==0.4.0)\n",
      "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cufft-cu11==10.9.0.58 (from torch>=1.13.0->peft==0.4.0)\n",
      "  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-curand-cu11==10.2.10.91 (from torch>=1.13.0->peft==0.4.0)\n",
      "  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m59.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusolver-cu11==11.4.0.1 (from torch>=1.13.0->peft==0.4.0)\n",
      "  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.6/102.6 MB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusparse-cu11==11.7.4.91 (from torch>=1.13.0->peft==0.4.0)\n",
      "  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nccl-cu11==2.14.3 (from torch>=1.13.0->peft==0.4.0)\n",
      "  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.1/177.1 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nvtx-cu11==11.7.91 (from torch>=1.13.0->peft==0.4.0)\n",
      "  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.6/98.6 kB\u001b[0m \u001b[31m33.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting triton==2.0.0 (from torch>=1.13.0->peft==0.4.0)\n",
      "  Downloading triton-2.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m47.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.13.0->peft==0.4.0) (67.8.0)\n",
      "Requirement already satisfied: wheel in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.13.0->peft==0.4.0) (0.38.4)\n",
      "Collecting cmake (from triton==2.0.0->torch>=1.13.0->peft==0.4.0)\n",
      "  Downloading cmake-3.27.0-py2.py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (26.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.0/26.0 MB\u001b[0m \u001b[31m73.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting lit (from triton==2.0.0->torch>=1.13.0->peft==0.4.0)\n",
      "  Downloading lit-16.0.6.tar.gz (153 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.7/153.7 kB\u001b[0m \u001b[31m41.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting multiprocess (from datasets==2.13.0)\n",
      "  Downloading multiprocess-0.70.14-py310-none-any.whl (134 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m34.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from pandas->datasets==2.13.0) (2.8.2)\n",
      "Collecting pytz>=2020.1 (from pandas->datasets==2.13.0)\n",
      "  Downloading pytz-2023.3-py2.py3-none-any.whl (502 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m502.3/502.3 kB\u001b[0m \u001b[31m88.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tzdata>=2022.1 (from pandas->datasets==2.13.0)\n",
      "  Downloading tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.8/341.8 kB\u001b[0m \u001b[31m75.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six>=1.5 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets==2.13.0) (1.16.0)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch>=1.13.0->peft==0.4.0)\n",
      "  Downloading MarkupSafe-2.1.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (28 kB)\n",
      "Collecting mpmath>=0.19 (from sympy->torch>=1.13.0->peft==0.4.0)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m49.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: lit\n",
      "  Building wheel for lit (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for lit: filename=lit-16.0.6-py3-none-any.whl size=93584 sha256=f8f018a73d10d07268ddc90d29c1c03615916d90476e7e88725110d19f683eda\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/ab/84/e4/5af8c76af9e5bee472e825f1451c18bb3b261d80a7b3ec7f8a\n",
      "Successfully built lit\n",
      "Installing collected packages: tokenizers, safetensors, pytz, mpmath, lit, cmake, bitsandbytes, xxhash, tzdata, sympy, regex, pyyaml, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, numpy, networkx, multidict, MarkupSafe, fsspec, frozenlist, filelock, dill, attrs, async-timeout, yarl, pyarrow, pandas, nvidia-cusolver-cu11, nvidia-cudnn-cu11, multiprocess, jinja2, huggingface-hub, aiosignal, transformers, aiohttp, datasets, triton, torch, accelerate, trl, peft\n",
      "Successfully installed MarkupSafe-2.1.3 accelerate-0.21.0 aiohttp-3.8.5 aiosignal-1.3.1 async-timeout-4.0.2 attrs-23.1.0 bitsandbytes-0.40.2 cmake-3.27.0 datasets-2.13.0 dill-0.3.6 filelock-3.12.2 frozenlist-1.4.0 fsspec-2023.6.0 huggingface-hub-0.16.4 jinja2-3.1.2 lit-16.0.6 mpmath-1.3.0 multidict-6.0.4 multiprocess-0.70.14 networkx-3.1 numpy-1.25.1 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 pandas-2.0.3 peft-0.4.0 pyarrow-12.0.1 pytz-2023.3 pyyaml-6.0.1 regex-2023.6.3 safetensors-0.3.1 sympy-1.12 tokenizers-0.13.3 torch-2.0.1 transformers-4.31.0 triton-2.0.0 trl-0.4.7 tzdata-2023.3 xxhash-3.3.0 yarl-1.9.2\n"
     ]
    }
   ],
   "source": [
    "!pip install \"transformers==4.31.0\" \"datasets==2.13.0\" \"peft==0.4.0\" \"accelerate==0.21.0\" \"bitsandbytes==0.40.2\" \"trl==0.4.7\" \"safetensors>=0.3.1\" --upgrade"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Databricks Dolly dataset `databricks/databricks-dolly-15k`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first load the dataset from the hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 8.20k/8.20k [00:00<00:00, 21.1MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/databricks--databricks-dolly-15k to /home/ec2-user/.cache/huggingface/datasets/databricks___json/databricks--databricks-dolly-15k-7427aa6e57c34282/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 13.1M/13.1M [00:00<00:00, 151MB/s]\n",
      "Downloading data files: 100%|██████████| 1/1 [00:00<00:00,  1.78it/s]\n",
      "Extracting data files: 100%|██████████| 1/1 [00:00<00:00, 1282.66it/s]\n",
      "                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /home/ec2-user/.cache/huggingface/datasets/databricks___json/databricks--databricks-dolly-15k-7427aa6e57c34282/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset from the hub\n",
    "dataset = load_dataset(\"databricks/databricks-dolly-15k\", split=\"train\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now take a look at the dataset. The data is in JSON format with the following schema:\n",
    "\n",
    "```js\n",
    "{\n",
    "    'instruction': 'I am trying to book a flight from Singapore to Sydney, what shall I do if the flight is too expensive?', \n",
    "    'context': '', \n",
    "    'response': 'You will have the option to choose from local Asian low-cost airlines such as Scoot, Jetstar, or AirAsia which would provide cheaper flights options.', \n",
    "    'category': 'general_qa'\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset size: 15011\n",
      "{'instruction': 'I am trying to book a flight from Singapore to Sydney, what shall I do if the flight is too expensive?', 'context': '', 'response': 'You will have the option to choose from local Asian low-cost airlines such as Scoot, Jetstar, or AirAsia which would provide cheaper flights options.', 'category': 'general_qa'}\n"
     ]
    }
   ],
   "source": [
    "from random import randrange\n",
    "\n",
    "print(f'dataset size: {len(dataset)}')\n",
    "print(dataset[randrange(len(dataset))])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a function to convert the data into a collection of tasks described by instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_instructions(sample):\n",
    "    return f\"\"\"### Instruction:\n",
    "Use the Input below to create an instruction, which could have been used to generate the Input using an LLM.\n",
    "\n",
    "### Input:\n",
    "{sample['response']}\n",
    "\n",
    "### Response:\n",
    "{sample['instruction']}\n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the `format_instructions` function with a random sample in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'instruction': 'When would a railway be considered a heritage railway?', 'context': \"A heritage railway or heritage railroad (US usage) is a railway operated as living history to re-create or preserve railway scenes of the past. Heritage railways are often old railway lines preserved in a state depicting a period (or periods) in the history of rail transport. The British Office of Rail and Road defines heritage railways as follows:...'lines of local interest', museum railways or tourist railways that have retained or assumed the character and appearance and operating practices of railways of former times. Several lines that operate in isolation provide genuine transport facilities, providing community links. Most lines constitute tourist or educational attractions in their own right. Much of the rolling stock and other equipment used on these systems is original and is of historic value in its own right. Many systems aim to replicate both the look and operating practices of historic former railways companies.\", 'response': 'Generally speaking, a railway can be considered a heritage railway when it assumes the appearance, characteristics, and operations of that railway from a period(s) in its past. These railways are often old and have been intentionally preserved to maintain its operating practices from prior times, purposely keeping equipment and processes original to a period in the past. Many of these heritage railways are preserved for educational purposes or as tourist attractions.', 'category': 'summarization'}\n",
      "### Instruction:\n",
      "Use the Input below to create an instruction, which could have been used to generate the Input using an LLM.\n",
      "\n",
      "### Input:\n",
      "Generally speaking, a railway can be considered a heritage railway when it assumes the appearance, characteristics, and operations of that railway from a period(s) in its past. These railways are often old and have been intentionally preserved to maintain its operating practices from prior times, purposely keeping equipment and processes original to a period in the past. Many of these heritage railways are preserved for educational purposes or as tourist attractions.\n",
      "\n",
      "### Response:\n",
      "When would a railway be considered a heritage railway?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from random import randrange\n",
    "\n",
    "sample_idx = randrange(len(dataset))\n",
    "print(dataset[sample_idx])\n",
    "print(format_instructions(dataset[sample_idx]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instruction-tune Llama 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flash Attention needs NVIDIA Ampere GPUs. See if we have it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/home/ec2-user/miniconda3/lib/python3.11/site-packages/torch/cuda/__init__.py\", line 381, in get_device_capability\n",
      "    prop = get_device_properties(device)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ec2-user/miniconda3/lib/python3.11/site-packages/torch/cuda/__init__.py\", line 395, in get_device_properties\n",
      "    _lazy_init()  # will define _get_device_properties\n",
      "    ^^^^^^^^^^^^\n",
      "  File \"/home/ec2-user/miniconda3/lib/python3.11/site-packages/torch/cuda/__init__.py\", line 247, in _lazy_init\n",
      "    torch._C._cuda_init()\n",
      "RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx\n"
     ]
    }
   ],
   "source": [
    "!python -c \"import torch; assert torch.cuda.get_device_capability()[0] >= 8, 'Hardware not supported for Flash Attention'\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The driver is missing. See how to install it here: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/install-nvidia-driver.html\n",
    "\n",
    "If you need to use a password for `ec2-user`, first change to root with `$ sudo su`, then `sudo passwd ec2-user` and type in a new password twice. After password is created, switch back to `ec2-user` by `$ su ec2-user`.\n",
    "\n",
    "For getting the driver from AWS (located in S3), add an IAM role to the EC2 which can access S3.\n",
    "\n",
    "After installation, checking the driver and GPU details should give you this:\n",
    "\n",
    "```bash\n",
    "$ nvidia-smi -q | head\n",
    "\n",
    "==============NVSMI LOG==============\n",
    "\n",
    "Timestamp                                 : Mon Jul 31 14:13:24 2023\n",
    "Driver Version                            : 535.54.03\n",
    "CUDA Version                              : 12.2\n",
    "\n",
    "Attached GPUs                             : 1\n",
    "GPU 00000000:00:1E.0\n",
    "    Product Name                          : NVIDIA A10G\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now this check should pass with no complaint and the pip installs would work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -c \"import torch; assert torch.cuda.get_device_capability()[0] >= 8, 'Hardware not supported for Flash Attention'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ninja\n",
      "  Downloading ninja-1.11.1-py2.py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (145 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m146.0/146.0 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging in /home/ec2-user/miniconda3/lib/python3.11/site-packages (23.0)\n",
      "Installing collected packages: ninja\n",
      "Successfully installed ninja-1.11.1\n"
     ]
    }
   ],
   "source": [
    "!pip install ninja packaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting flash-attn\n",
      "  Downloading flash_attn-2.0.2.tar.gz (4.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m54.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[13 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m torch.__version__  = 2.0.1+cu117\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m Traceback (most recent call last):\n",
      "  \u001b[31m   \u001b[0m   File \"<string>\", line 2, in <module>\n",
      "  \u001b[31m   \u001b[0m   File \"<pip-setuptools-caller>\", line 34, in <module>\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-install-6hpibdd5/flash-attn_ec1990251ecc4243a47398f0a6794995/setup.py\", line 108, in <module>\n",
      "  \u001b[31m   \u001b[0m     raise_if_cuda_home_none(\"flash_attn\")\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-install-6hpibdd5/flash-attn_ec1990251ecc4243a47398f0a6794995/setup.py\", line 55, in raise_if_cuda_home_none\n",
      "  \u001b[31m   \u001b[0m     raise RuntimeError(\n",
      "  \u001b[31m   \u001b[0m RuntimeError: flash_attn was requested, but nvcc was not found.  Are you sure your environment has nvcc available?  If you're installing within a container from https://hub.docker.com/r/pytorch/pytorch, only images whose names contain 'devel' will provide nvcc.\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[?25h\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
      "\u001b[31m╰─>\u001b[0m See above for output.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
      "\u001b[1;36mhint\u001b[0m: See above for details.\n"
     ]
    }
   ],
   "source": [
    "!MAX_JOBS=4 pip install flash-attn --no-build-isolation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I need a newer version of CUDA?\n",
    " \n",
    "https://developer.nvidia.com/cuda-downloads?target_os=Linux&target_arch=x86_64&Distribution=CentOS&target_version=7&target_type=runfile_local\n",
    "\n",
    "Or install cuda with `conda`?\n",
    "\n",
    "https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html\n",
    "\n",
    "```bash\n",
    "$ conda install cuda -c nvidia\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting flash-attn\n",
      "  Downloading flash_attn-2.0.2.tar.gz (4.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m56.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: torch in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from flash-attn) (2.0.1)\n",
      "Collecting einops (from flash-attn)\n",
      "  Downloading einops-0.6.1-py3-none-any.whl (42 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m322.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from flash-attn) (23.0)\n",
      "Requirement already satisfied: ninja in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from flash-attn) (1.11.1)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from torch->flash-attn) (3.12.2)\n",
      "Requirement already satisfied: typing-extensions in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from torch->flash-attn) (4.7.1)\n",
      "Requirement already satisfied: sympy in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from torch->flash-attn) (1.12)\n",
      "Requirement already satisfied: networkx in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from torch->flash-attn) (3.1)\n",
      "Requirement already satisfied: jinja2 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from torch->flash-attn) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from torch->flash-attn) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from torch->flash-attn) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from torch->flash-attn) (11.7.101)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from torch->flash-attn) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from torch->flash-attn) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from torch->flash-attn) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from torch->flash-attn) (10.2.10.91)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from torch->flash-attn) (11.4.0.1)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from torch->flash-attn) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from torch->flash-attn) (2.14.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from torch->flash-attn) (11.7.91)\n",
      "Requirement already satisfied: triton==2.0.0 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from torch->flash-attn) (2.0.0)\n",
      "Requirement already satisfied: setuptools in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch->flash-attn) (67.8.0)\n",
      "Requirement already satisfied: wheel in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch->flash-attn) (0.38.4)\n",
      "Requirement already satisfied: cmake in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from triton==2.0.0->torch->flash-attn) (3.27.0)\n",
      "Requirement already satisfied: lit in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from triton==2.0.0->torch->flash-attn) (16.0.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from jinja2->torch->flash-attn) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from sympy->torch->flash-attn) (1.3.0)\n",
      "Building wheels for collected packages: flash-attn\n",
      "  Building wheel for flash-attn (setup.py) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[114 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m torch.__version__  = 2.0.1+cu117\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m fatal: not a git repository (or any parent up to mount point /)\n",
      "  \u001b[31m   \u001b[0m Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n",
      "  \u001b[31m   \u001b[0m running bdist_wheel\n",
      "  \u001b[31m   \u001b[0m running build\n",
      "  \u001b[31m   \u001b[0m running build_py\n",
      "  \u001b[31m   \u001b[0m creating build\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-311\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-311/flash_attn\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/tmp.py -> build/lib.linux-x86_64-cpython-311/flash_attn\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/rotary.py -> build/lib.linux-x86_64-cpython-311/flash_attn\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/fused_softmax.py -> build/lib.linux-x86_64-cpython-311/flash_attn\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/flash_blocksparse_attn_interface.py -> build/lib.linux-x86_64-cpython-311/flash_attn\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/flash_blocksparse_attention.py -> build/lib.linux-x86_64-cpython-311/flash_attn\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/flash_attn_triton_varlen.py -> build/lib.linux-x86_64-cpython-311/flash_attn\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/flash_attn_triton_tmp_og.py -> build/lib.linux-x86_64-cpython-311/flash_attn\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/flash_attn_triton_tmp.py -> build/lib.linux-x86_64-cpython-311/flash_attn\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/flash_attn_triton_single_query.py -> build/lib.linux-x86_64-cpython-311/flash_attn\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/flash_attn_triton_og.py -> build/lib.linux-x86_64-cpython-311/flash_attn\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/flash_attn_triton.py -> build/lib.linux-x86_64-cpython-311/flash_attn\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/flash_attn_interface.py -> build/lib.linux-x86_64-cpython-311/flash_attn\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/flash_attention.py -> build/lib.linux-x86_64-cpython-311/flash_attn\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/fav2_interface.py -> build/lib.linux-x86_64-cpython-311/flash_attn\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/bert_padding.py -> build/lib.linux-x86_64-cpython-311/flash_attn\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/attention_kernl.py -> build/lib.linux-x86_64-cpython-311/flash_attn\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/__init__.py -> build/lib.linux-x86_64-cpython-311/flash_attn\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-311/flash_attn/utils\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/utils/pretrained.py -> build/lib.linux-x86_64-cpython-311/flash_attn/utils\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/utils/generation.py -> build/lib.linux-x86_64-cpython-311/flash_attn/utils\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/utils/distributed.py -> build/lib.linux-x86_64-cpython-311/flash_attn/utils\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/utils/benchmark.py -> build/lib.linux-x86_64-cpython-311/flash_attn/utils\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/utils/__init__.py -> build/lib.linux-x86_64-cpython-311/flash_attn/utils\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-311/flash_attn/ops\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/ops/rms_norm.py -> build/lib.linux-x86_64-cpython-311/flash_attn/ops\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/ops/layer_norm.py -> build/lib.linux-x86_64-cpython-311/flash_attn/ops\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/ops/gelu_activation.py -> build/lib.linux-x86_64-cpython-311/flash_attn/ops\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/ops/fused_dense.py -> build/lib.linux-x86_64-cpython-311/flash_attn/ops\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/ops/activations.py -> build/lib.linux-x86_64-cpython-311/flash_attn/ops\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/ops/__init__.py -> build/lib.linux-x86_64-cpython-311/flash_attn/ops\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-311/flash_attn/modules\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/modules/mlp.py -> build/lib.linux-x86_64-cpython-311/flash_attn/modules\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/modules/mha.py -> build/lib.linux-x86_64-cpython-311/flash_attn/modules\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/modules/embedding.py -> build/lib.linux-x86_64-cpython-311/flash_attn/modules\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/modules/block.py -> build/lib.linux-x86_64-cpython-311/flash_attn/modules\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/modules/__init__.py -> build/lib.linux-x86_64-cpython-311/flash_attn/modules\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-311/flash_attn/models\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/models/vit.py -> build/lib.linux-x86_64-cpython-311/flash_attn/models\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/models/opt.py -> build/lib.linux-x86_64-cpython-311/flash_attn/models\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/models/llama.py -> build/lib.linux-x86_64-cpython-311/flash_attn/models\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/models/gptj.py -> build/lib.linux-x86_64-cpython-311/flash_attn/models\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/models/gpt_neox.py -> build/lib.linux-x86_64-cpython-311/flash_attn/models\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/models/gpt.py -> build/lib.linux-x86_64-cpython-311/flash_attn/models\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/models/falcon.py -> build/lib.linux-x86_64-cpython-311/flash_attn/models\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/models/bert.py -> build/lib.linux-x86_64-cpython-311/flash_attn/models\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/models/__init__.py -> build/lib.linux-x86_64-cpython-311/flash_attn/models\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-311/flash_attn/losses\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/losses/cross_entropy.py -> build/lib.linux-x86_64-cpython-311/flash_attn/losses\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/losses/__init__.py -> build/lib.linux-x86_64-cpython-311/flash_attn/losses\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-311/flash_attn/layers\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/layers/rotary.py -> build/lib.linux-x86_64-cpython-311/flash_attn/layers\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/layers/patch_embed.py -> build/lib.linux-x86_64-cpython-311/flash_attn/layers\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/layers/__init__.py -> build/lib.linux-x86_64-cpython-311/flash_attn/layers\n",
      "  \u001b[31m   \u001b[0m running build_ext\n",
      "  \u001b[31m   \u001b[0m Traceback (most recent call last):\n",
      "  \u001b[31m   \u001b[0m   File \"<string>\", line 2, in <module>\n",
      "  \u001b[31m   \u001b[0m   File \"<pip-setuptools-caller>\", line 34, in <module>\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-install-dqyoeqaw/flash-attn_3da6cbfb7242422193035ce84449d9d4/setup.py\", line 201, in <module>\n",
      "  \u001b[31m   \u001b[0m     setup(\n",
      "  \u001b[31m   \u001b[0m   File \"/home/ec2-user/miniconda3/lib/python3.11/site-packages/setuptools/__init__.py\", line 107, in setup\n",
      "  \u001b[31m   \u001b[0m     return distutils.core.setup(**attrs)\n",
      "  \u001b[31m   \u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/home/ec2-user/miniconda3/lib/python3.11/site-packages/setuptools/_distutils/core.py\", line 185, in setup\n",
      "  \u001b[31m   \u001b[0m     return run_commands(dist)\n",
      "  \u001b[31m   \u001b[0m            ^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/home/ec2-user/miniconda3/lib/python3.11/site-packages/setuptools/_distutils/core.py\", line 201, in run_commands\n",
      "  \u001b[31m   \u001b[0m     dist.run_commands()\n",
      "  \u001b[31m   \u001b[0m   File \"/home/ec2-user/miniconda3/lib/python3.11/site-packages/setuptools/_distutils/dist.py\", line 969, in run_commands\n",
      "  \u001b[31m   \u001b[0m     self.run_command(cmd)\n",
      "  \u001b[31m   \u001b[0m   File \"/home/ec2-user/miniconda3/lib/python3.11/site-packages/setuptools/dist.py\", line 1244, in run_command\n",
      "  \u001b[31m   \u001b[0m     super().run_command(command)\n",
      "  \u001b[31m   \u001b[0m   File \"/home/ec2-user/miniconda3/lib/python3.11/site-packages/setuptools/_distutils/dist.py\", line 988, in run_command\n",
      "  \u001b[31m   \u001b[0m     cmd_obj.run()\n",
      "  \u001b[31m   \u001b[0m   File \"/home/ec2-user/miniconda3/lib/python3.11/site-packages/wheel/bdist_wheel.py\", line 325, in run\n",
      "  \u001b[31m   \u001b[0m     self.run_command(\"build\")\n",
      "  \u001b[31m   \u001b[0m   File \"/home/ec2-user/miniconda3/lib/python3.11/site-packages/setuptools/_distutils/cmd.py\", line 318, in run_command\n",
      "  \u001b[31m   \u001b[0m     self.distribution.run_command(command)\n",
      "  \u001b[31m   \u001b[0m   File \"/home/ec2-user/miniconda3/lib/python3.11/site-packages/setuptools/dist.py\", line 1244, in run_command\n",
      "  \u001b[31m   \u001b[0m     super().run_command(command)\n",
      "  \u001b[31m   \u001b[0m   File \"/home/ec2-user/miniconda3/lib/python3.11/site-packages/setuptools/_distutils/dist.py\", line 988, in run_command\n",
      "  \u001b[31m   \u001b[0m     cmd_obj.run()\n",
      "  \u001b[31m   \u001b[0m   File \"/home/ec2-user/miniconda3/lib/python3.11/site-packages/setuptools/_distutils/command/build.py\", line 131, in run\n",
      "  \u001b[31m   \u001b[0m     self.run_command(cmd_name)\n",
      "  \u001b[31m   \u001b[0m   File \"/home/ec2-user/miniconda3/lib/python3.11/site-packages/setuptools/_distutils/cmd.py\", line 318, in run_command\n",
      "  \u001b[31m   \u001b[0m     self.distribution.run_command(command)\n",
      "  \u001b[31m   \u001b[0m   File \"/home/ec2-user/miniconda3/lib/python3.11/site-packages/setuptools/dist.py\", line 1244, in run_command\n",
      "  \u001b[31m   \u001b[0m     super().run_command(command)\n",
      "  \u001b[31m   \u001b[0m   File \"/home/ec2-user/miniconda3/lib/python3.11/site-packages/setuptools/_distutils/dist.py\", line 988, in run_command\n",
      "  \u001b[31m   \u001b[0m     cmd_obj.run()\n",
      "  \u001b[31m   \u001b[0m   File \"/home/ec2-user/miniconda3/lib/python3.11/site-packages/setuptools/command/build_ext.py\", line 84, in run\n",
      "  \u001b[31m   \u001b[0m     _build_ext.run(self)\n",
      "  \u001b[31m   \u001b[0m   File \"/home/ec2-user/miniconda3/lib/python3.11/site-packages/setuptools/_distutils/command/build_ext.py\", line 345, in run\n",
      "  \u001b[31m   \u001b[0m     self.build_extensions()\n",
      "  \u001b[31m   \u001b[0m   File \"/home/ec2-user/miniconda3/lib/python3.11/site-packages/torch/utils/cpp_extension.py\", line 499, in build_extensions\n",
      "  \u001b[31m   \u001b[0m     _check_cuda_version(compiler_name, compiler_version)\n",
      "  \u001b[31m   \u001b[0m   File \"/home/ec2-user/miniconda3/lib/python3.11/site-packages/torch/utils/cpp_extension.py\", line 387, in _check_cuda_version\n",
      "  \u001b[31m   \u001b[0m     raise RuntimeError(CUDA_MISMATCH_MESSAGE.format(cuda_str_version, torch.version.cuda))\n",
      "  \u001b[31m   \u001b[0m RuntimeError:\n",
      "  \u001b[31m   \u001b[0m The detected CUDA version (12.2) mismatches the version that was used to compile\n",
      "  \u001b[31m   \u001b[0m PyTorch (11.7). Please make sure to use the same CUDA versions.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[31m  ERROR: Failed building wheel for flash-attn\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[?25h  Running setup.py clean for flash-attn\n",
      "Failed to build flash-attn\n",
      "\u001b[31mERROR: Could not build wheels for flash-attn, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!MAX_JOBS=4 pip install flash-attn --no-build-isolation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then I got this.\n",
    "```bash\n",
    "The detected CUDA version (12.2) mismatches the version that was used to compile\n",
    "      PyTorch (11.7). Please make sure to use the same CUDA versions.\n",
    "```\n",
    "So let me try install CUDA pytorch 11.7\n",
    "\n",
    "```bash\n",
    "# uninstall the current CUDA 12.2 \n",
    "$ conda remove cuda\n",
    "# install 11.7\n",
    "$ conda install cuda -c nvidia/label/cuda-11.7.0\n",
    "```\n",
    "\n",
    "After this, I still needed to install `typing-extensions`:\n",
    "\n",
    "```bash\n",
    "$ pip install typing-extensions\n",
    "```\n",
    "\n",
    "Then it finally started to build \n",
    "```bash\n",
    "Building wheels for collected packages: flash-attn\n",
    "  Building wheel for flash-attn (setup.py) ... \\\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
